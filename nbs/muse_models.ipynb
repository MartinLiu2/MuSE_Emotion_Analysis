{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import numpy as np\r\n",
    "import torch.nn.functional as F\r\n",
    "import itertools\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import sampler\r\n",
    "import datasets\r\n",
    "from earlystopping import EarlyStopping\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score, recall_score\r\n",
    "from torch.autograd  import  Function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Acoustic Branch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inputs for acoustic branch will be N x 40 where N [1,33]  \n",
    "Time step: (2, 10) (seconds?)  \n",
    "N: relative duration after feature extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "class AcousticNet(nn.Module):\r\n",
    "    def __init__(self, num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2):\r\n",
    "        super(AcousticNet, self).__init__()\r\n",
    "        self.num_conv_layers = num_conv_layers\r\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\r\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\r\n",
    "        self.conv3 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\r\n",
    "        self.conv4 = nn.Conv1d(in_channels=conv_width, out_channels=conv_width, kernel_size=kernel_size, padding = kernel_size - 1)\r\n",
    "        self.convs = [self.conv1, self.conv2, self.conv3, self.conv4]\r\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = 2)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        \r\n",
    "        self.gru = nn.GRU(input_size=conv_width,hidden_size=32,num_layers=num_gru_layers) # 19 is hardcoded\r\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = torch.transpose(x, 1, 2) \r\n",
    "        print(x.shape)\r\n",
    "        for i in range(self.num_conv_layers):\r\n",
    "            x = self.relu(self.max_pool(self.convs[i](x)))\r\n",
    "        x = torch.transpose(x, 1, 2) \r\n",
    "        x, _ = self.gru(x)\r\n",
    "        x = torch.transpose(x, 1, 2)\r\n",
    "        x = F.adaptive_avg_pool1d(x,1)[:, :, -1]\r\n",
    "#         x = self.mean_pool(x)\r\n",
    "        return x\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# Test dummy input\r\n",
    "net = AcousticNet(num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2)\r\n",
    "batch_size = 8\r\n",
    "n_acoustic_channels = 40\r\n",
    "duration_acoustic = 1232\r\n",
    "test_vec = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\r\n",
    "output = net(test_vec)\r\n",
    "print(f'Shape of output: {output.shape}')\r\n",
    "# assert output.shape[-1] == 16"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([8, 40, 1232])\n",
      "Shape of output: torch.Size([8, 32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lexical Branch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# implement GRU (or transformer)\r\n",
    "class LexicalNet(nn.Module):\r\n",
    "    def __init__(self, num_gru_layers = 2):\r\n",
    "        super(LexicalNet, self).__init__()\r\n",
    "        # implement GRU (or transformer)\r\n",
    "        self.gru = nn.GRU(input_size=768,hidden_size=32,num_layers=num_gru_layers)\r\n",
    "        self.mean_pool = nn.AvgPool1d(kernel_size=2) \r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x, _ = self.gru(x)\r\n",
    "#         x = self.mean_pool(x)\r\n",
    "        x = self.flatten(x)\r\n",
    "#         print(x.shape)\r\n",
    "        return x\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# Test dummy input\r\n",
    "net = LexicalNet(num_gru_layers = 2)\r\n",
    "batch_size = 8\r\n",
    "test_vec = torch.randn(batch_size, 1, 768)\r\n",
    "output = net(test_vec)\r\n",
    "# assert output.shape[-1] == 16\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Master branch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# class GRL(Function):\r\n",
    "#     @staticmethod\r\n",
    "#     def forward(self,x):\r\n",
    "#         return x\r\n",
    "#     @staticmethod\r\n",
    "#     def backward(self,grad_output):\r\n",
    "#         grad_input = grad_output.neg()\r\n",
    "#         return grad_input"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "class GradientReversalFunction(Function):\r\n",
    "    \"\"\"\r\n",
    "    Gradient Reversal Layer from:\r\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\r\n",
    "    Forward pass is the identity function. In the backward pass,\r\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def forward(ctx, x, lambda_):\r\n",
    "        ctx.lambda_ = lambda_\r\n",
    "        return x.clone()\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def backward(ctx, grads):\r\n",
    "        lambda_ = ctx.lambda_\r\n",
    "        lambda_ = grads.new_tensor(lambda_)\r\n",
    "        dx = -lambda_ * grads\r\n",
    "        return dx, None\r\n",
    "    \r\n",
    "class GradientReversal(torch.nn.Module):\r\n",
    "    def __init__(self, lambda_=1):\r\n",
    "        super(GradientReversal, self).__init__()\r\n",
    "        self.lambda_ = lambda_\r\n",
    "        print(self.lambda_)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "class MasterNet(nn.Module):\r\n",
    "    def __init__(self, acoustic_modality = True, lexical_modality = True, visual_modality = False,\r\n",
    "                 num_conv_layers = 3, kernel_size = 2, conv_width = 32, num_gru_layers = 2,\r\n",
    "                 num_dense_layers = 1, dense_layer_width = 32, grl_lambda = .3):\r\n",
    "        super(MasterNet, self).__init__()\r\n",
    "        \r\n",
    "        self.acoustic_modality = acoustic_modality\r\n",
    "        self.lexical_modality = lexical_modality\r\n",
    "        self.visual_modality = visual_modality\r\n",
    "        \r\n",
    "        self.acoustic_model = AcousticNet(num_conv_layers = num_conv_layers, kernel_size = kernel_size, \r\n",
    "                                     conv_width = conv_width, num_gru_layers = num_gru_layers)\r\n",
    "        self.lexical_model = LexicalNet(num_gru_layers = 2)\r\n",
    "        \r\n",
    "        # emotion classifier\r\n",
    "#         self.dense1_emo = nn.Linear()\r\n",
    "#         self.dense2_emo = nn.Linear()\r\n",
    "        \r\n",
    "        width = 0 # width of the FC layers\r\n",
    "        if self.acoustic_modality:\r\n",
    "            width += 32\r\n",
    "        if self.visual_modality:\r\n",
    "            width += 0 # to implement\r\n",
    "        if self.lexical_modality:\r\n",
    "            width += 32\r\n",
    "            \r\n",
    "        self.fc_1 = nn.Linear(width, dense_layer_width)\r\n",
    "        self.fc_2 = nn.Linear(dense_layer_width, 3)\r\n",
    "        self.softmax = nn.Softmax(dim=1)\r\n",
    "\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "#         # To implement   \r\n",
    "#         if num_dense_layers == 2:\r\n",
    "#             self.fc = nn.Sequential()\r\n",
    "#             self.linear_1 = nn.Linear(width, dense_layer_width)\r\n",
    "#         else:\r\n",
    "#             self.fc = \r\n",
    "        \r\n",
    "        # confound classifier -- to implement\r\n",
    "        self.grl = GradientReversal(lambda_ = grl_lambda)\r\n",
    "        self.dense_conv = nn.Linear(width, 2)\r\n",
    "#         self.dense2_con = None\r\n",
    "        \r\n",
    "        \r\n",
    "    def forward_a(self, x_a):\r\n",
    "        x = x_a\r\n",
    "        x = self.acoustic_model(x)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def forward_l(self, x_l):\r\n",
    "        x = torch.unsqueeze(x_l, dim = 1)\r\n",
    "        x = self.lexical_model(x)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def forward_v(self, x_v):\r\n",
    "        x = x_v\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def encoder(self, x_v, x_a, x_l):\r\n",
    "        if self.visual_modality:\r\n",
    "            x_v = self.forward_v(x_v)\r\n",
    "        if self.acoustic_modality:\r\n",
    "            x_a = self.forward_a(x_a)\r\n",
    "        if self.lexical_modality:\r\n",
    "            x_l = self.forward_l(x_l)\r\n",
    "        \r\n",
    "        if self.visual_modality:\r\n",
    "            if self.acoustic_modality:\r\n",
    "                if self.lexical_modality:\r\n",
    "                    x = torch.cat((x_v, x_a, x_l), 1)\r\n",
    "                else:\r\n",
    "                    x = torch.cat((x_v, x_a), 1)\r\n",
    "            else:\r\n",
    "                if self.lexical_modality:\r\n",
    "                    x = torch.cat((x_v, x_l), 1)\r\n",
    "                else:\r\n",
    "                    x = x_v\r\n",
    "        else:\r\n",
    "            if self.acoustic_modality:\r\n",
    "                if self.lexical_modality:\r\n",
    "                    x = torch.cat((x_a, x_l), 1)\r\n",
    "                else:\r\n",
    "                    x = x_a\r\n",
    "            else:\r\n",
    "                x = x_l\r\n",
    "        return x\r\n",
    "\r\n",
    "    def confound_model(self, x):\r\n",
    "#         x = self.grl.apply(x)\r\n",
    "        x = self.grl(x)\r\n",
    "        x = self.dense_conv(x)\r\n",
    "        x = self.softmax(x)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    # For emotion\r\n",
    "    def recognizer(self, x):\r\n",
    "#         print(x.shape)\r\n",
    "        x = self.relu(self.fc_1(x))\r\n",
    "        x = self.fc_2(x)\r\n",
    "        x = self.softmax(x)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def forward(self, x_v, x_a, x_l):\r\n",
    "        x = self.encoder(x_v, x_a, x_l)\r\n",
    "        emotion_output = self.recognizer(x)\r\n",
    "        confound_output = self.confound_model(x)\r\n",
    "        \r\n",
    "        return emotion_output, confound_output\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# Test dummy input\r\n",
    "net = MasterNet()\r\n",
    "batch_size = 8\r\n",
    "n_acoustic_channels = 40\r\n",
    "duration_acoustic = 1232\r\n",
    "acoustic_features = torch.randn(batch_size, duration_acoustic, n_acoustic_channels) # samples x features (or channels) x N (relative duration)\r\n",
    "# lexical_features = torch.randn(batch_size, 1, 300)\r\n",
    "lexical_features = torch.randn(batch_size, 768)\r\n",
    "visual_features = None\r\n",
    "emotion_output, stress_output = net(visual_features, acoustic_features, lexical_features)\r\n",
    "print(f'Shape of emotion output: {emotion_output.shape}')\r\n",
    "print(f'Shape of stress output: {stress_output.shape}')\r\n",
    "print(emotion_output)\r\n",
    "print(stress_output)\r\n",
    "# assert output.shape[-1] == 16"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n",
      "torch.Size([8, 40, 1232])\n",
      "Shape of emotion output: torch.Size([8, 3])\n",
      "Shape of stress output: torch.Size([8, 2])\n",
      "tensor([[0.2723, 0.3767, 0.3510],\n",
      "        [0.2759, 0.3801, 0.3440],\n",
      "        [0.2686, 0.3792, 0.3522],\n",
      "        [0.2673, 0.3724, 0.3603],\n",
      "        [0.2659, 0.3780, 0.3561],\n",
      "        [0.2704, 0.3751, 0.3545],\n",
      "        [0.2772, 0.3787, 0.3441],\n",
      "        [0.2718, 0.3895, 0.3387]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4901, 0.5099],\n",
      "        [0.5345, 0.4655],\n",
      "        [0.5524, 0.4476],\n",
      "        [0.5523, 0.4477],\n",
      "        [0.5602, 0.4398],\n",
      "        [0.5633, 0.4367],\n",
      "        [0.5103, 0.4897],\n",
      "        [0.4923, 0.5077]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# Use specific GPU\r\n",
    "def get_device():\r\n",
    "    if torch.cuda.is_available():  \r\n",
    "        dev = \"cuda:0\" \r\n",
    "    else:  \r\n",
    "        dev = \"cpu\"  \r\n",
    "    return torch.device(dev)\r\n",
    "device = get_device()\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "def train_one_folder(model, folder = 0, epochs = 1, verbose = False, learning_rate = 1e-4, patience = 5):\r\n",
    "    # Use specific GPU\r\n",
    "    device = get_device()\r\n",
    "\r\n",
    "    # Dataloaders    \r\n",
    "    train_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/train.csv'\r\n",
    "    train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\r\n",
    "    test_dataset_file_path = '../dataset/IEMOCAP/' + str(folder) + '/test.csv'\r\n",
    "    test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\r\n",
    "\r\n",
    "    # Model, optimizer and loss function\r\n",
    "    init_weights(model)\r\n",
    "    for param in emotion_recognizer.parameters():\r\n",
    "        param.requires_grad = True\r\n",
    "    model.to(device)\r\n",
    "\r\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "    lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\r\n",
    "\r\n",
    "    criterion = torch.nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    best_acc = 0.\r\n",
    "    best_uar = 0.\r\n",
    "    es = EarlyStopping(patience=patience)\r\n",
    "\r\n",
    "    # Train and validate\r\n",
    "    for epoch in range(epochs):\r\n",
    "        if verbose:\r\n",
    "            print('epoch: {}/{}'.format(epoch + 1, epochs))\r\n",
    "\r\n",
    "        train_loss, train_acc = train(train_loader, model,\r\n",
    "                                        optimizer, criterion, device)\r\n",
    "        test_loss, test_acc, test_uar = test(test_loader, model,\r\n",
    "                                                criterion, device)\r\n",
    "\r\n",
    "        if verbose:\r\n",
    "            print('train_emotion_loss: {0:.5f}'.format(train_loss['emotion_loss']),\r\n",
    "                  'train_emotion_acc: {0:.3f}'.format(train_acc['emotion_acc']),\r\n",
    "                  'train_confound_loss: {0:.5f}'.format(train_loss['confound_loss']),\r\n",
    "                  'train_confound_acc: {0:.3f}'.format(train_acc['confound_acc']),\r\n",
    "                  'test_emotion_loss: {0:.5f}'.format(test_loss['emotion_loss']),\r\n",
    "                  'test_emotion_acc: {0:.3f}'.format(test_acc['emotion_acc']),\r\n",
    "                  'test_confound_loss: {0:.5f}'.format(test_loss['confound_loss']),\r\n",
    "                  'test_confound_acc: {0:.3f}'.format(test_acc['confound_acc']),\r\n",
    "                  'test_emotion_uar: {0:.3f}'.format(test_uar['emotion_uar']),\r\n",
    "                  'test_confound_uar: {0:.3f}'.format(test_uar['confound_uar']))\r\n",
    "\r\n",
    "        lr_schedule.step(test_loss['loss'])\r\n",
    "\r\n",
    "#         os.makedirs(os.path.join(opt.logger_path, opt.source_domain), exist_ok=True)\r\n",
    "\r\n",
    "#         model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'checkpoint.pth.tar')\r\n",
    "#         state = {'epoch': epoch+1, 'emotion_recognizer': emotion_recognizer.state_dict(), 'opt': opt}\r\n",
    "#         torch.save(state, model_file_name)\r\n",
    "\r\n",
    "        if test_acc['emotion_acc'] > best_acc:\r\n",
    "#             model_file_name = os.path.join(opt.logger_path, opt.source_domain, 'model.pth.tar')\r\n",
    "#             torch.save(state, model_file_name)\r\n",
    "\r\n",
    "            best_acc = test_acc['emotion_acc']\r\n",
    "\r\n",
    "        if test_uar['emotion_uar'] > best_uar:\r\n",
    "            best_uar = test_uar['emotion_uar']\r\n",
    "\r\n",
    "        if es.step(test_loss['emotion_loss']):\r\n",
    "            break\r\n",
    "\r\n",
    "    return best_acc, best_uar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "\r\n",
    "def train_baseline(train_loader, model, optimizer, criterion, device, verbose = False):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    running_loss = 0.\r\n",
    "    running_acc = 0.\r\n",
    "\r\n",
    "    groundtruth = []\r\n",
    "    prediction = []\r\n",
    "\r\n",
    "    for i, train_data in enumerate(train_loader):\r\n",
    "        visual_features, _, acoustic_features, _, lexical_features, _, _, a_labels, _, _ = train_data # UPDATE\r\n",
    "\r\n",
    "        visual_features = visual_features.to(device)\r\n",
    "        acoustic_features = acoustic_features.to(device)\r\n",
    "        lexical_features = lexical_features.to(device)\r\n",
    "\r\n",
    "        labels = a_labels.to(device)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        emotion_output, stress_output = model(visual_features, acoustic_features, lexical_features)\r\n",
    "\r\n",
    "        emotion_loss = criterion(emotion_output, labels)\r\n",
    "#         stress_loss = criterion(stress_output, stress_labels)\r\n",
    "\r\n",
    "        emotion_loss.backward()\r\n",
    "#         stress_loss.backward()\r\n",
    "        \r\n",
    "        optimizer.step() # do we need two optimizers?\r\n",
    "        \r\n",
    "        running_loss += emotion_loss.item()\r\n",
    "\r\n",
    "        groundtruth.append(labels.tolist())\r\n",
    "        predictions = emotion_output.argmax(dim=1, keepdim=True)\r\n",
    "        prediction.append(predictions.view_as(labels).tolist())\r\n",
    "\r\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\r\n",
    "            print('.', flush=True, end='')\r\n",
    "            \r\n",
    "    train_loss = running_loss / len(train_loader)\r\n",
    "\r\n",
    "    groundtruth = list(itertools.chain.from_iterable(groundtruth))\r\n",
    "    prediction = list(itertools.chain.from_iterable(prediction))\r\n",
    "\r\n",
    "    train_acc = accuracy_score(prediction, groundtruth)\r\n",
    "\r\n",
    "    return train_loss, train_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "def train(train_loader, model, optimizer, criterion, device, verbose = False):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    running_loss = 0.\r\n",
    "    emotion_running_loss = 0.\r\n",
    "    confound_running_loss = 0.\r\n",
    "    running_acc = 0.\r\n",
    "\r\n",
    "    emotion_groundtruth = []\r\n",
    "    emotion_prediction = []\r\n",
    "    \r\n",
    "    confound_groundtruth = []\r\n",
    "    confound_prediction = []\r\n",
    "\r\n",
    "    for i, train_data in enumerate(train_loader):\r\n",
    "        acoustic_features, _, lexical_features, v_labels, a_labels, s_labels = train_data\r\n",
    "\r\n",
    "        acoustic_features = acoustic_features.to(device)\r\n",
    "        lexical_features = lexical_features.to(device)\r\n",
    "\r\n",
    "        emotion_labels = a_labels.to(device)\r\n",
    "        confound_labels = s_labels.to(device)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        emotion_predictions, confound_predictions = model(None, acoustic_features, lexical_features)\r\n",
    "\r\n",
    "        emotion_loss = criterion(emotion_predictions, emotion_labels)\r\n",
    "        confound_loss = criterion(confound_predictions, confound_labels)\r\n",
    "        loss = emotion_loss + confound_loss\r\n",
    "        \r\n",
    "        loss.backward()\r\n",
    "#         emotion_loss.backward()\r\n",
    "#         confound_loss.backward()\r\n",
    "        \r\n",
    "        optimizer.step() # do we need two optimizers?\r\n",
    "        \r\n",
    "        emotion_running_loss += emotion_loss.item()\r\n",
    "        confound_running_loss += confound_loss.item()\r\n",
    "        running_loss += emotion_running_loss + confound_running_loss\r\n",
    "\r\n",
    "        emotion_groundtruth.append(emotion_labels.tolist())\r\n",
    "        emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\r\n",
    "        emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\r\n",
    "        \r\n",
    "        confound_groundtruth.append(confound_labels.tolist())\r\n",
    "        confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\r\n",
    "        confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\r\n",
    "\r\n",
    "        if verbose and i > 0 and int(len(train_loader) / 10) > 0 and i % (int(len(train_loader) / 10)) == 0:\r\n",
    "            print('.', flush=True, end='')\r\n",
    "        \r\n",
    "    emotion_loss = emotion_running_loss / len(train_loader)\r\n",
    "    confound_loss = confound_running_loss / len(train_loader)\r\n",
    "    loss = running_loss / len(train_loader)\r\n",
    "    train_loss = {'emotion_loss': emotion_loss,\r\n",
    "                  'confound_loss': confound_loss,\r\n",
    "                  'loss': loss\r\n",
    "                 }\r\n",
    "\r\n",
    "    emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\r\n",
    "    emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\r\n",
    "    \r\n",
    "    confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\r\n",
    "    confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\r\n",
    "\r\n",
    "    emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\r\n",
    "    confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\r\n",
    "    avg_acc = (emotion_acc + confound_acc) / 2\r\n",
    "    \r\n",
    "    train_acc = {'emotion_acc': emotion_acc,\r\n",
    "                  'confound_acc': confound_acc\r\n",
    "                }\r\n",
    "\r\n",
    "    return train_loss, train_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def test_baseline(test_loader, model, criterion, device):\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    running_loss = 0.\r\n",
    "    running_acc = 0.\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        groundtruth = []\r\n",
    "        prediction = []\r\n",
    "\r\n",
    "        for i, test_data in enumerate(test_loader):\r\n",
    "            visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, d_labels, _ = test_data # UPDATE\r\n",
    "\r\n",
    "            visual_features = visual_features.to(device)\r\n",
    "            acoustic_features = acoustic_features.to(device)\r\n",
    "            lexical_features = lexical_features.to(device)\r\n",
    "\r\n",
    "            labels = a_labels.to(device)\r\n",
    "\r\n",
    "            emotion_predictions, confound_predictions = model(visual_features, acoustic_features, lexical_features)\r\n",
    "            loss = criterion(emotion_predictions, labels)\r\n",
    "\r\n",
    "            running_loss += loss.item()\r\n",
    "\r\n",
    "            groundtruth.append(labels.tolist())\r\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\r\n",
    "            prediction.append(emotion_predictions.view_as(labels).tolist())\r\n",
    "\r\n",
    "        test_loss = running_loss / len(test_loader)\r\n",
    "\r\n",
    "        groundtruth = list(itertools.chain.from_iterable(groundtruth))\r\n",
    "        prediction = list(itertools.chain.from_iterable(prediction))\r\n",
    "\r\n",
    "        test_acc = accuracy_score(prediction, groundtruth)\r\n",
    "        test_uar = recall_score(prediction, groundtruth, average='macro')\r\n",
    "\r\n",
    "        return test_loss, test_acc, test_uar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "def test(test_loader, model, criterion, device):\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    running_loss = 0.\r\n",
    "    emotion_running_loss = 0.\r\n",
    "    confound_running_loss = 0.\r\n",
    "    running_acc = 0.\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        emotion_groundtruth = []\r\n",
    "        emotion_prediction = []\r\n",
    "\r\n",
    "        # How should I change the confound_predictions?\r\n",
    "        confound_groundtruth = []\r\n",
    "        confound_prediction = []\r\n",
    "\r\n",
    "        for i, test_data in enumerate(test_loader):\r\n",
    "            # visual_features, _, acoustic_features, _, lexical_features, _, v_labels, a_labels, _, s_labels_labels, _ = test_data\r\n",
    "            acoustic_features, lexical_features, v_labels, a_labels, s_labels = test_data\r\n",
    "\r\n",
    "            # visual_features = visual_features.to(device)\r\n",
    "            acoustic_features = acoustic_features.to(device)\r\n",
    "            lexical_features = lexical_features.to(device)\r\n",
    "\r\n",
    "            emotion_labels = a_labels.to(device)\r\n",
    "            confound_labels = s_labels.to(device)  # Need to figure out what confounder to use for MuSE \r\n",
    "\r\n",
    "            emotion_predictions, confound_predictions = model(acoustic_features, lexical_features) # Removed visual_features\r\n",
    "            \r\n",
    "            emotion_loss = criterion(emotion_predictions, emotion_labels)\r\n",
    "            confound_loss = criterion(confound_predictions, confound_labels)\r\n",
    "            loss = emotion_loss + confound_loss\r\n",
    "\r\n",
    "            emotion_running_loss += emotion_loss.item()\r\n",
    "            confound_running_loss += confound_loss.item()\r\n",
    "            running_loss += emotion_running_loss + confound_running_loss\r\n",
    "\r\n",
    "            emotion_groundtruth.append(emotion_labels.tolist())\r\n",
    "            emotion_predictions = emotion_predictions.argmax(dim=1, keepdim=True)\r\n",
    "            emotion_prediction.append(emotion_predictions.view_as(emotion_labels).tolist())\r\n",
    "\r\n",
    "            confound_groundtruth.append(confound_labels.tolist())\r\n",
    "            confound_predictions = confound_predictions.argmax(dim=1, keepdim=True)\r\n",
    "            confound_prediction.append(confound_predictions.view_as(confound_labels).tolist())\r\n",
    "\r\n",
    "        emotion_loss = emotion_running_loss / len(train_loader)\r\n",
    "        confound_loss = confound_running_loss / len(train_loader)\r\n",
    "        loss = running_loss / len(train_loader)\r\n",
    "        test_loss = {'emotion_loss': emotion_loss,\r\n",
    "                     'confound_loss': confound_loss,\r\n",
    "                     'loss': loss\r\n",
    "                    }\r\n",
    "\r\n",
    "        emotion_groundtruth = list(itertools.chain.from_iterable(emotion_groundtruth))\r\n",
    "        emotion_prediction = list(itertools.chain.from_iterable(emotion_prediction))\r\n",
    "\r\n",
    "        confound_groundtruth = list(itertools.chain.from_iterable(confound_groundtruth))\r\n",
    "        confound_prediction = list(itertools.chain.from_iterable(confound_prediction))\r\n",
    "\r\n",
    "        emotion_acc = accuracy_score(emotion_prediction, emotion_groundtruth)\r\n",
    "        confound_acc = accuracy_score(confound_prediction, confound_groundtruth)\r\n",
    "        avg_acc = (emotion_acc + confound_acc) / 2\r\n",
    "        test_acc = {'emotion_acc': emotion_acc,\r\n",
    "                    'confound_acc': confound_acc\r\n",
    "                   }\r\n",
    "        \r\n",
    "        emotion_uar = recall_score(emotion_prediction, emotion_groundtruth, average='macro')\r\n",
    "        confound_uar = recall_score(confound_prediction, confound_groundtruth, average='macro')\r\n",
    "\r\n",
    "        test_uar = {'emotion_uar': emotion_uar,\r\n",
    "                    'confound_uar': confound_uar\r\n",
    "                   }\r\n",
    "\r\n",
    "        return test_loss, test_acc, test_uar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def init_weights(m):\r\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\r\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\r\n",
    "        if m.bias is not None:\r\n",
    "            m.bias.data.fill_(0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "emotion_recognizer = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False, grl_lambda = 10)\r\n",
    "init_weights(emotion_recognizer)\r\n",
    "for param in emotion_recognizer.parameters():\r\n",
    "    param.requires_grad = True\r\n",
    "emotion_recognizer.to(device)\r\n",
    "\r\n",
    "learning_rate = 1e-4\r\n",
    "optimizer = torch.optim.Adam(emotion_recognizer.parameters(), lr=learning_rate)\r\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\r\n",
    "criterion = torch.nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "train_dataset_file_path = '../dataset/MuSE/0/train.csv'\r\n",
    "train_loader = datasets.get_dataloader(train_dataset_file_path, 'train')\r\n",
    "test_dataset_file_path = '../dataset/MuSE/0/test.csv'\r\n",
    "test_loader = datasets.get_dataloader(test_dataset_file_path, 'test')\r\n",
    "\r\n",
    "emotion_loss = []\r\n",
    "confound_loss = []\r\n",
    "loss = []\r\n",
    "epochs = 50\r\n",
    "for epoch in range(epochs):\r\n",
    "    train_loss, train_acc = train(train_loader, emotion_recognizer, optimizer, criterion, device)\r\n",
    "    emotion_loss.append(train_loss['emotion_loss'])\r\n",
    "    confound_loss.append(train_loss['confound_loss'])\r\n",
    "    loss.append(train_loss['loss'])\r\n",
    "    print(f'Train loss: {train_loss}')\r\n",
    "    print(f'Train acc: {train_acc}')\r\n",
    "\r\n",
    "# plt.plot(range(epochs), loss, label = 'train loss')\r\n",
    "plt.plot(range(epochs), emotion_loss, label = 'emotion loss')\r\n",
    "plt.plot(range(epochs), confound_loss, label = 'confound loss')\r\n",
    "plt.legend()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-14f6bd6d0ca7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion_recognizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0memotion_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'emotion_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mconfound_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'confound_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-dec33b9c2d5d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, optimizer, criterion, device, verbose)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0memotion_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfound_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macoustic_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0memotion_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-6020195ca5b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_v, x_a, x_l)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0memotion_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mconfound_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfound_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-6020195ca5b9>\u001b[0m in \u001b[0;36mencoder\u001b[1;34m(self, x_v, x_a, x_l)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mx_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macoustic_modality\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mx_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexical_modality\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mx_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_l\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-6020195ca5b9>\u001b[0m in \u001b[0;36mforward_a\u001b[1;34m(self, x_a)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macoustic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-db04ac6bd40d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_conv_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_loss, test_acc, test_uar = test(test_loader, emotion_recognizer, criterion, device)\r\n",
    "print(f'Test loss: {test_loss}')\r\n",
    "print(f'Test acc: {test_acc}')\r\n",
    "print(f'Test uar: {test_uar}')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-98882667450c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_uar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion_recognizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test loss: {test_loss}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test acc: {test_acc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test uar: {test_uar}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-8c7cba35cd5e>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(test_loader, model, criterion, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mconfound_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mvisual_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macoustic_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;31m# UPDATE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    877\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plt.plot(range(epochs), loss, label = 'train loss')\r\n",
    "plt.plot(range(epochs), emotion_loss, label = 'emotion loss')\r\n",
    "plt.plot(range(epochs), confound_loss, label = 'confound loss')\r\n",
    "plt.legend()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3de9285698de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# plt.plot(range(epochs), loss, label = 'train loss')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'emotion loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfound_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'confound loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "acc = []\r\n",
    "uar = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\r\n",
    "best_acc, best_uar = train_one_folder(model, folder = 0, verbose = True, epochs = 10)\r\n",
    "acc.append(best_acc)\r\n",
    "uar.append(best_uar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n",
      "epoch: 1/10\n",
      "train_emotion_loss: 1.09418 train_emotion_acc: 0.375 train_confound_loss: 0.70122 train_confound_acc: 0.479 test_emotion_loss: 0.23824 test_emotion_acc: 0.484 test_confound_loss: 0.15995 test_confound_acc: 0.411 test_emotion_uar: 0.444 test_confound_uar: 0.407\n",
      "epoch: 2/10\n",
      "train_emotion_loss: 1.04321 train_emotion_acc: 0.491 train_confound_loss: 0.69855 train_confound_acc: 0.471 test_emotion_loss: 0.21883 test_emotion_acc: 0.612 test_confound_loss: 0.15433 test_confound_acc: 0.474 test_emotion_uar: 0.535 test_confound_uar: 0.495\n",
      "epoch: 3/10\n",
      "train_emotion_loss: 0.99746 train_emotion_acc: 0.527 train_confound_loss: 0.69062 train_confound_acc: 0.533 test_emotion_loss: 0.21356 test_emotion_acc: 0.603 test_confound_loss: 0.15075 test_confound_acc: 0.626 test_emotion_uar: 0.532 test_confound_uar: 0.629\n",
      "epoch: 4/10\n",
      "train_emotion_loss: 0.97856 train_emotion_acc: 0.548 train_confound_loss: 0.68800 train_confound_acc: 0.553 test_emotion_loss: 0.20994 test_emotion_acc: 0.620 test_confound_loss: 0.15058 test_confound_acc: 0.624 test_emotion_uar: 0.557 test_confound_uar: 0.619\n",
      "epoch: 5/10\n",
      "train_emotion_loss: 0.96569 train_emotion_acc: 0.568 train_confound_loss: 0.68768 train_confound_acc: 0.552 test_emotion_loss: 0.20969 test_emotion_acc: 0.601 test_confound_loss: 0.15031 test_confound_acc: 0.618 test_emotion_uar: 0.567 test_confound_uar: 0.612\n",
      "epoch: 6/10\n",
      "train_emotion_loss: 0.95518 train_emotion_acc: 0.580 train_confound_loss: 0.68737 train_confound_acc: 0.555 test_emotion_loss: 0.21088 test_emotion_acc: 0.585 test_confound_loss: 0.15143 test_confound_acc: 0.592 test_emotion_uar: 0.580 test_confound_uar: 0.590\n",
      "epoch: 7/10\n",
      "train_emotion_loss: 0.93980 train_emotion_acc: 0.602 train_confound_loss: 0.68638 train_confound_acc: 0.553 test_emotion_loss: 0.21060 test_emotion_acc: 0.580 test_confound_loss: 0.15113 test_confound_acc: 0.589 test_emotion_uar: 0.580 test_confound_uar: 0.569\n",
      "epoch: 8/10\n",
      "train_emotion_loss: 0.94373 train_emotion_acc: 0.595 train_confound_loss: 0.68874 train_confound_acc: 0.536 test_emotion_loss: 0.21044 test_emotion_acc: 0.581 test_confound_loss: 0.15125 test_confound_acc: 0.584 test_emotion_uar: 0.577 test_confound_uar: 0.567\n",
      "epoch: 9/10\n",
      "train_emotion_loss: 0.93226 train_emotion_acc: 0.608 train_confound_loss: 0.68767 train_confound_acc: 0.537 test_emotion_loss: 0.21015 test_emotion_acc: 0.583 test_confound_loss: 0.15107 test_confound_acc: 0.585 test_emotion_uar: 0.574 test_confound_uar: 0.564\n",
      "epoch: 10/10\n",
      "train_emotion_loss: 0.93222 train_emotion_acc: 0.607 train_confound_loss: 0.68893 train_confound_acc: 0.534 test_emotion_loss: 0.20993 test_emotion_acc: 0.584 test_confound_loss: 0.15100 test_confound_acc: 0.586 test_emotion_uar: 0.573 test_confound_uar: 0.565\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\r\n",
    "best_acc, best_uar = train_one_folder(model, folder = 1, verbose = True, epochs = 10)\r\n",
    "acc.append(best_acc)\r\n",
    "uar.append(best_uar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n",
      "epoch: 1/10\n",
      "train_emotion_loss: 1.09286 train_emotion_acc: 0.366 train_confound_loss: 0.70725 train_confound_acc: 0.488 test_emotion_loss: 0.23269 test_emotion_acc: 0.536 test_confound_loss: 0.16578 test_confound_acc: 0.433 test_emotion_uar: 0.524 test_confound_uar: 0.300\n",
      "epoch: 2/10\n",
      "train_emotion_loss: 1.03608 train_emotion_acc: 0.490 train_confound_loss: 0.70085 train_confound_acc: 0.486 test_emotion_loss: 0.21610 test_emotion_acc: 0.584 test_confound_loss: 0.15146 test_confound_acc: 0.647 test_emotion_uar: 0.515 test_confound_uar: 0.641\n",
      "epoch: 3/10\n",
      "train_emotion_loss: 0.99721 train_emotion_acc: 0.525 train_confound_loss: 0.68720 train_confound_acc: 0.574 test_emotion_loss: 0.21237 test_emotion_acc: 0.585 test_confound_loss: 0.14817 test_confound_acc: 0.677 test_emotion_uar: 0.511 test_confound_uar: 0.741\n",
      "epoch: 4/10\n",
      "train_emotion_loss: 0.98384 train_emotion_acc: 0.543 train_confound_loss: 0.68905 train_confound_acc: 0.542 test_emotion_loss: 0.20931 test_emotion_acc: 0.592 test_confound_loss: 0.15112 test_confound_acc: 0.580 test_emotion_uar: 0.526 test_confound_uar: 0.566\n",
      "epoch: 5/10\n",
      "train_emotion_loss: 0.97043 train_emotion_acc: 0.562 train_confound_loss: 0.69250 train_confound_acc: 0.512 test_emotion_loss: 0.21223 test_emotion_acc: 0.563 test_confound_loss: 0.15247 test_confound_acc: 0.527 test_emotion_uar: 0.523 test_confound_uar: 0.534\n",
      "epoch: 6/10\n",
      "train_emotion_loss: 0.95996 train_emotion_acc: 0.573 train_confound_loss: 0.69115 train_confound_acc: 0.527 test_emotion_loss: 0.20834 test_emotion_acc: 0.589 test_confound_loss: 0.15141 test_confound_acc: 0.566 test_emotion_uar: 0.538 test_confound_uar: 0.577\n",
      "epoch: 7/10\n",
      "train_emotion_loss: 0.95111 train_emotion_acc: 0.587 train_confound_loss: 0.68811 train_confound_acc: 0.539 test_emotion_loss: 0.20823 test_emotion_acc: 0.590 test_confound_loss: 0.14953 test_confound_acc: 0.613 test_emotion_uar: 0.548 test_confound_uar: 0.603\n",
      "epoch: 8/10\n",
      "train_emotion_loss: 0.93502 train_emotion_acc: 0.603 train_confound_loss: 0.68799 train_confound_acc: 0.545 test_emotion_loss: 0.20979 test_emotion_acc: 0.576 test_confound_loss: 0.14964 test_confound_acc: 0.601 test_emotion_uar: 0.556 test_confound_uar: 0.590\n",
      "epoch: 9/10\n",
      "train_emotion_loss: 0.93346 train_emotion_acc: 0.603 train_confound_loss: 0.68769 train_confound_acc: 0.542 test_emotion_loss: 0.21031 test_emotion_acc: 0.565 test_confound_loss: 0.15021 test_confound_acc: 0.586 test_emotion_uar: 0.538 test_confound_uar: 0.572\n",
      "epoch: 10/10\n",
      "train_emotion_loss: 0.91863 train_emotion_acc: 0.630 train_confound_loss: 0.68838 train_confound_acc: 0.540 test_emotion_loss: 0.21040 test_emotion_acc: 0.562 test_confound_loss: 0.15042 test_confound_acc: 0.581 test_emotion_uar: 0.535 test_confound_uar: 0.567\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\r\n",
    "best_acc, best_uar = train_one_folder(model, folder = 2, verbose = True, epochs = 10)\r\n",
    "acc.append(best_acc)\r\n",
    "uar.append(best_uar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n",
      "epoch: 1/10\n",
      "train_emotion_loss: 1.09586 train_emotion_acc: 0.367 train_confound_loss: 0.70769 train_confound_acc: 0.411 test_emotion_loss: 0.27894 test_emotion_acc: 0.480 test_confound_loss: 0.19097 test_confound_acc: 0.381 test_emotion_uar: 0.666 test_confound_uar: 0.293\n",
      "epoch: 2/10\n",
      "train_emotion_loss: 1.04326 train_emotion_acc: 0.491 train_confound_loss: 0.70410 train_confound_acc: 0.449 test_emotion_loss: 0.25858 test_emotion_acc: 0.563 test_confound_loss: 0.17748 test_confound_acc: 0.561 test_emotion_uar: 0.389 test_confound_uar: 0.756\n",
      "epoch: 3/10\n",
      "train_emotion_loss: 1.00316 train_emotion_acc: 0.523 train_confound_loss: 0.68362 train_confound_acc: 0.560 test_emotion_loss: 0.25328 test_emotion_acc: 0.574 test_confound_loss: 0.17434 test_confound_acc: 0.601 test_emotion_uar: 0.516 test_confound_uar: 0.724\n",
      "epoch: 4/10\n",
      "train_emotion_loss: 0.99133 train_emotion_acc: 0.534 train_confound_loss: 0.68311 train_confound_acc: 0.572 test_emotion_loss: 0.25110 test_emotion_acc: 0.576 test_confound_loss: 0.17599 test_confound_acc: 0.571 test_emotion_uar: 0.538 test_confound_uar: 0.645\n",
      "epoch: 5/10\n",
      "train_emotion_loss: 0.96493 train_emotion_acc: 0.569 train_confound_loss: 0.68576 train_confound_acc: 0.555 test_emotion_loss: 0.25047 test_emotion_acc: 0.559 test_confound_loss: 0.17784 test_confound_acc: 0.551 test_emotion_uar: 0.530 test_confound_uar: 0.562\n",
      "epoch: 6/10\n",
      "train_emotion_loss: 0.96091 train_emotion_acc: 0.573 train_confound_loss: 0.68867 train_confound_acc: 0.546 test_emotion_loss: 0.25046 test_emotion_acc: 0.559 test_confound_loss: 0.17984 test_confound_acc: 0.521 test_emotion_uar: 0.551 test_confound_uar: 0.525\n",
      "epoch: 7/10\n",
      "train_emotion_loss: 0.95306 train_emotion_acc: 0.584 train_confound_loss: 0.68868 train_confound_acc: 0.541 test_emotion_loss: 0.25044 test_emotion_acc: 0.560 test_confound_loss: 0.17997 test_confound_acc: 0.515 test_emotion_uar: 0.549 test_confound_uar: 0.515\n",
      "epoch: 8/10\n",
      "train_emotion_loss: 0.95385 train_emotion_acc: 0.581 train_confound_loss: 0.68926 train_confound_acc: 0.534 test_emotion_loss: 0.25071 test_emotion_acc: 0.558 test_confound_loss: 0.18007 test_confound_acc: 0.515 test_emotion_uar: 0.548 test_confound_uar: 0.514\n",
      "epoch: 9/10\n",
      "train_emotion_loss: 0.94732 train_emotion_acc: 0.586 train_confound_loss: 0.69094 train_confound_acc: 0.525 test_emotion_loss: 0.25047 test_emotion_acc: 0.560 test_confound_loss: 0.18005 test_confound_acc: 0.517 test_emotion_uar: 0.549 test_confound_uar: 0.518\n",
      "epoch: 10/10\n",
      "train_emotion_loss: 0.95026 train_emotion_acc: 0.586 train_confound_loss: 0.68899 train_confound_acc: 0.538 test_emotion_loss: 0.25040 test_emotion_acc: 0.560 test_confound_loss: 0.18005 test_confound_acc: 0.519 test_emotion_uar: 0.548 test_confound_uar: 0.521\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 3, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n",
      "epoch: 1/10\n",
      "train_emotion_loss: 1.09400 train_emotion_acc: 0.366 train_confound_loss: 0.69898 train_confound_acc: 0.499 test_emotion_loss: 0.27083 test_emotion_acc: 0.506 test_confound_loss: 0.18330 test_confound_acc: 0.418 test_emotion_uar: 0.335 test_confound_uar: 0.272\n",
      "epoch: 2/10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/brandon/src/stressed_emotion/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_emotion_loss: 1.03172 train_emotion_acc: 0.498 train_confound_loss: 0.69526 train_confound_acc: 0.528 test_emotion_loss: 0.25574 test_emotion_acc: 0.539 test_confound_loss: 0.17665 test_confound_acc: 0.497 test_emotion_uar: 0.437 test_confound_uar: 0.605\n",
      "epoch: 3/10\n",
      "train_emotion_loss: 0.99116 train_emotion_acc: 0.538 train_confound_loss: 0.68868 train_confound_acc: 0.544 test_emotion_loss: 0.25314 test_emotion_acc: 0.532 test_confound_loss: 0.17365 test_confound_acc: 0.612 test_emotion_uar: 0.445 test_confound_uar: 0.657\n",
      "epoch: 4/10\n",
      "train_emotion_loss: 0.96983 train_emotion_acc: 0.557 train_confound_loss: 0.68726 train_confound_acc: 0.559 test_emotion_loss: 0.25108 test_emotion_acc: 0.537 test_confound_loss: 0.17413 test_confound_acc: 0.585 test_emotion_uar: 0.483 test_confound_uar: 0.623\n",
      "epoch: 5/10\n",
      "train_emotion_loss: 0.95895 train_emotion_acc: 0.569 train_confound_loss: 0.68482 train_confound_acc: 0.566 test_emotion_loss: 0.25197 test_emotion_acc: 0.536 test_confound_loss: 0.17561 test_confound_acc: 0.524 test_emotion_uar: 0.501 test_confound_uar: 0.564\n",
      "epoch: 6/10\n",
      "train_emotion_loss: 0.95184 train_emotion_acc: 0.587 train_confound_loss: 0.68631 train_confound_acc: 0.547 test_emotion_loss: 0.25090 test_emotion_acc: 0.541 test_confound_loss: 0.17624 test_confound_acc: 0.541 test_emotion_uar: 0.503 test_confound_uar: 0.557\n",
      "epoch: 7/10\n",
      "train_emotion_loss: 0.93909 train_emotion_acc: 0.601 train_confound_loss: 0.68663 train_confound_acc: 0.557 test_emotion_loss: 0.25098 test_emotion_acc: 0.538 test_confound_loss: 0.17640 test_confound_acc: 0.537 test_emotion_uar: 0.502 test_confound_uar: 0.554\n",
      "epoch: 8/10\n",
      "train_emotion_loss: 0.94064 train_emotion_acc: 0.596 train_confound_loss: 0.68802 train_confound_acc: 0.543 test_emotion_loss: 0.25106 test_emotion_acc: 0.538 test_confound_loss: 0.17646 test_confound_acc: 0.537 test_emotion_uar: 0.503 test_confound_uar: 0.553\n",
      "epoch: 9/10\n",
      "train_emotion_loss: 0.93710 train_emotion_acc: 0.601 train_confound_loss: 0.68574 train_confound_acc: 0.557 test_emotion_loss: 0.25110 test_emotion_acc: 0.536 test_confound_loss: 0.17648 test_confound_acc: 0.535 test_emotion_uar: 0.502 test_confound_uar: 0.551\n",
      "epoch: 10/10\n",
      "train_emotion_loss: 0.93905 train_emotion_acc: 0.600 train_confound_loss: 0.68655 train_confound_acc: 0.550 test_emotion_loss: 0.25107 test_emotion_acc: 0.537 test_confound_loss: 0.17648 test_confound_acc: 0.536 test_emotion_uar: 0.503 test_confound_uar: 0.552\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = MasterNet(acoustic_modality = True, lexical_modality = True, visual_modality = False)\n",
    "best_acc, best_uar = train_one_folder(model, folder = 4, verbose = True, epochs = 10)\n",
    "acc.append(best_acc)\n",
    "uar.append(best_uar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3\n",
      "epoch: 1/10\n",
      "train_emotion_loss: 1.09515 train_emotion_acc: 0.356 train_confound_loss: 0.69853 train_confound_acc: 0.464 test_emotion_loss: 0.28314 test_emotion_acc: 0.505 test_confound_loss: 0.19179 test_confound_acc: 0.232 test_emotion_uar: 0.411 test_confound_uar: 0.232\n",
      "epoch: 2/10\n",
      "train_emotion_loss: 1.03523 train_emotion_acc: 0.497 train_confound_loss: 0.70149 train_confound_acc: 0.435 test_emotion_loss: 0.26472 test_emotion_acc: 0.538 test_confound_loss: 0.18564 test_confound_acc: 0.454 test_emotion_uar: 0.421 test_confound_uar: 0.449\n",
      "epoch: 3/10\n",
      "train_emotion_loss: 1.00367 train_emotion_acc: 0.516 train_confound_loss: 0.68288 train_confound_acc: 0.565 test_emotion_loss: 0.26100 test_emotion_acc: 0.544 test_confound_loss: 0.18001 test_confound_acc: 0.527 test_emotion_uar: 0.466 test_confound_uar: 0.597\n",
      "epoch: 4/10\n",
      "train_emotion_loss: 0.98333 train_emotion_acc: 0.543 train_confound_loss: 0.68287 train_confound_acc: 0.557 test_emotion_loss: 0.26050 test_emotion_acc: 0.535 test_confound_loss: 0.18216 test_confound_acc: 0.509 test_emotion_uar: 0.481 test_confound_uar: 0.539\n",
      "epoch: 5/10\n",
      "train_emotion_loss: 0.97809 train_emotion_acc: 0.547 train_confound_loss: 0.68765 train_confound_acc: 0.544 test_emotion_loss: 0.26158 test_emotion_acc: 0.524 test_confound_loss: 0.18446 test_confound_acc: 0.479 test_emotion_uar: 0.471 test_confound_uar: 0.489\n",
      "epoch: 6/10\n",
      "train_emotion_loss: 0.97093 train_emotion_acc: 0.558 train_confound_loss: 0.68870 train_confound_acc: 0.540 test_emotion_loss: 0.25974 test_emotion_acc: 0.535 test_confound_loss: 0.18398 test_confound_acc: 0.484 test_emotion_uar: 0.485 test_confound_uar: 0.497\n",
      "epoch: 7/10\n",
      "train_emotion_loss: 0.96299 train_emotion_acc: 0.573 train_confound_loss: 0.68756 train_confound_acc: 0.547 test_emotion_loss: 0.25992 test_emotion_acc: 0.537 test_confound_loss: 0.18396 test_confound_acc: 0.485 test_emotion_uar: 0.490 test_confound_uar: 0.498\n",
      "epoch: 8/10\n",
      "train_emotion_loss: 0.96158 train_emotion_acc: 0.570 train_confound_loss: 0.68681 train_confound_acc: 0.550 test_emotion_loss: 0.25996 test_emotion_acc: 0.536 test_confound_loss: 0.18397 test_confound_acc: 0.484 test_emotion_uar: 0.488 test_confound_uar: 0.497\n",
      "epoch: 9/10\n",
      "train_emotion_loss: 0.96780 train_emotion_acc: 0.568 train_confound_loss: 0.68775 train_confound_acc: 0.547 test_emotion_loss: 0.25999 test_emotion_acc: 0.534 test_confound_loss: 0.18397 test_confound_acc: 0.485 test_emotion_uar: 0.485 test_confound_uar: 0.499\n",
      "epoch: 10/10\n",
      "train_emotion_loss: 0.96902 train_emotion_acc: 0.568 train_confound_loss: 0.68840 train_confound_acc: 0.537 test_emotion_loss: 0.25998 test_emotion_acc: 0.534 test_confound_loss: 0.18397 test_confound_acc: 0.485 test_emotion_uar: 0.485 test_confound_uar: 0.499\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(5),acc)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8+yak3AAAACXBIWXMAAAsTAAALEwEAmpwYAAANRUlEQVR4nO3dcYjf913H8edrl0XFDfdHThhJ2AXNBsecnZ5ZoaCjtnC1kgirI5GNFTqDsGClQ01RAsZ/3AbVf/LH4lYdas1qFTndSSiuIoqtuW61eonRM1ZzQei1q04Rm517+8f9On9efsnvm+R39/M+93zAwe/7/X643/tLyZMv3999f01VIUna+t407gEkSaNh0CWpEQZdkhph0CWpEQZdkhqxY1xvvGvXrpqamhrX20vSlvT888+/UlWTg46NLehTU1MsLCyM6+0laUtK8k/XO+YtF0lqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxNieFL0dU8e/MO4RRualX7p/3CNIaoRX6JLUCIMuSY3oFPQks0kuJllKcvw6az6Y5HySxSRPjHZMSdIwQ++hJ5kATgH3AsvAuSRzVXW+b81+4FHgrqp6Lcm3b9TAkqTBulyhHwCWqupSVV0FzgCH1q35ceBUVb0GUFUvj3ZMSdIwXYK+G7jct73c29fvncA7k/x5kmeTzA76RUmOJllIsrCysnJrE0uSBhrVh6I7gP3A+4EjwK8medv6RVV1uqpmqmpmcnLg/3BDknSLugT9CrC3b3tPb1+/ZWCuqr5WVf8I/B1rgZckbZIuDxadA/Yn2cdayA8DP7Zuze+zdmX+a0l2sXYL5tII51SfVh6s8qEqabSGXqFX1SpwDDgLXACerKrFJCeTHOwtOwu8muQ88Azw01X16kYNLUm6VqdH/6tqHphft+9E3+sCHun9SJLGwCdFJakRBl2SGrElv21R21crHwiDHwpr9LxCl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoSP/kvaEvzah+EMurSFtBI1v8dmY3jLRZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqRGdgp5kNsnFJEtJjg84/mCSlSQv9H4+OvpRJUk3MvS7XJJMAKeAe4Fl4FySuao6v27p56vq2AbMKEnqoMsV+gFgqaouVdVV4AxwaGPHkiTdrC5B3w1c7tte7u1b7wNJXkzyVJK9g35RkqNJFpIsrKys3MK4kqTrGdWHon8ATFXVe4Cngc8NWlRVp6tqpqpmJicnR/TWkiToFvQrQP8V957evm+oqler6vXe5meA7x3NeJKkrroE/RywP8m+JDuBw8Bc/4Ikb+/bPAhcGN2IkqQuhv6VS1WtJjkGnAUmgMerajHJSWChquaAn0xyEFgFvgI8uIEzS5IG6PS/oKuqeWB+3b4Tfa8fBR4d7WiSpJvhk6KS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmN6BT0JLNJLiZZSnL8Bus+kKSSzIxuRElSF0ODnmQCOAXcB0wDR5JMD1j3VuBh4LlRDylJGq7LFfoBYKmqLlXVVeAMcGjAul8EPgH81wjnkyR11CXou4HLfdvLvX3fkOR7gL1V9YUb/aIkR5MsJFlYWVm56WElSdd32x+KJnkT8Bjw8WFrq+p0Vc1U1czk5OTtvrUkqU+XoF8B9vZt7+nte8NbgXcDf5LkJeBOYM4PRiVpc3UJ+jlgf5J9SXYCh4G5Nw5W1b9V1a6qmqqqKeBZ4GBVLWzIxJKkgYYGvapWgWPAWeAC8GRVLSY5meTgRg8oSepmR5dFVTUPzK/bd+I6a99/+2NJkm6WT4pKUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1olPQk8wmuZhkKcnxAcd/IslfJ3khyZ8lmR79qJKkGxka9CQTwCngPmAaODIg2E9U1XdV1R3AJ4HHRj2oJOnGulyhHwCWqupSVV0FzgCH+hdU1Vf7Nr8VqNGNKEnqYkeHNbuBy33by8D71i9K8jHgEWAncPdIppMkdTayD0Wr6lRVfQfws8DPD1qT5GiShSQLKysro3prSRLdgn4F2Nu3vae373rOAD8y6EBVna6qmaqamZyc7DykJGm4LkE/B+xPsi/JTuAwMNe/IMn+vs37gb8f3YiSpC6G3kOvqtUkx4CzwATweFUtJjkJLFTVHHAsyT3A14DXgI9s5NCSpGt1+VCUqpoH5tftO9H3+uERzyVJukk+KSpJjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjegU9CSzSS4mWUpyfMDxR5KcT/Jikj9O8o7RjypJupGhQU8yAZwC7gOmgSNJptct+zIwU1XvAZ4CPjnqQSVJN9blCv0AsFRVl6rqKnAGONS/oKqeqar/7G0+C+wZ7ZiSpGG6BH03cLlve7m373oeAv5o0IEkR5MsJFlYWVnpPqUkaaiRfiia5EPADPCpQcer6nRVzVTVzOTk5CjfWpK2vR0d1lwB9vZt7+nt+z+S3AP8HPADVfX6aMaTJHXV5Qr9HLA/yb4kO4HDwFz/giTvBT4NHKyql0c/piRpmKFBr6pV4BhwFrgAPFlVi0lOJjnYW/Yp4C3A7yR5IcncdX6dJGmDdLnlQlXNA/Pr9p3oe33PiOeSJN0knxSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ5lNcjHJUpLjA45/f5IvJVlN8sDox5QkDTM06EkmgFPAfcA0cCTJ9Lpl/ww8CDwx6gElSd3s6LDmALBUVZcAkpwBDgHn31hQVS/1jn19A2aUJHXQ5ZbLbuBy3/Zyb99NS3I0yUKShZWVlVv5FZKk69jUD0Wr6nRVzVTVzOTk5Ga+tSQ1r0vQrwB7+7b39PZJkv4f6RL0c8D+JPuS7AQOA3MbO5Yk6WYNDXpVrQLHgLPABeDJqlpMcjLJQYAk35dkGfhR4NNJFjdyaEnStbr8lQtVNQ/Mr9t3ou/1OdZuxUiSxsQnRSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEZ2CnmQ2ycUkS0mODzj+TUk+3zv+XJKpkU8qSbqhoUFPMgGcAu4DpoEjSabXLXsIeK2qvhP4ZeATox5UknRjXa7QDwBLVXWpqq4CZ4BD69YcAj7Xe/0U8INJMroxJUnDpKpuvCB5AJitqo/2tj8MvK+qjvWt+ZvemuXe9j/01ryy7ncdBY72Nt8FXBzViWyQXcArQ1e1yXPfvrbz+W+Fc39HVU0OOrBjM6eoqtPA6c18z9uRZKGqZsY9xzh47tvz3GF7n/9WP/cut1yuAHv7tvf09g1ck2QH8G3Aq6MYUJLUTZegnwP2J9mXZCdwGJhbt2YO+Ejv9QPAF2vYvRxJ0kgNveVSVatJjgFngQng8apaTHISWKiqOeCzwG8kWQK+wlr0W7Blbg9tAM99+9rO57+lz33oh6KSpK3BJ0UlqREGXZIaYdAHGPZVBy1L8niSl3vPFmwrSfYmeSbJ+SSLSR4e90ybJck3J/nLJH/VO/dfGPdM45BkIsmXk/zhuGe5FQZ9nY5fddCyXwdmxz3EmKwCH6+qaeBO4GPb6L/968DdVfXdwB3AbJI7xzvSWDwMXBj3ELfKoF+ry1cdNKuq/pS1v1TadqrqX6rqS73X/87aP+zd451qc9Sa/+htvrn3s63+YiLJHuB+4DPjnuVWGfRr7QYu920vs03+Uet/9b4x9L3Ac2MeZdP0bje8ALwMPF1V2+bce34F+Bng62Oe45YZdGmdJG8Bfhf4qar66rjn2SxV9d9VdQdrT4MfSPLuMY+0aZL8MPByVT0/7lluh0G/VpevOlCjkryZtZj/VlX93rjnGYeq+lfgGbbXZyl3AQeTvMTabda7k/zmeEe6eQb9Wl2+6kAN6n3l82eBC1X12Ljn2UxJJpO8rff6W4B7gb8d61CbqKoerao9VTXF2r/5L1bVh8Y81k0z6OtU1SrwxlcdXACerKrF8U61eZL8NvAXwLuSLCd5aNwzbaK7gA+zdnX2Qu/nh8Y91CZ5O/BMkhdZu6h5uqq25J/ubWc++i9JjfAKXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa8T9fq0jKHG/09AAAAABJRU5ErkJggg=="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.bar(range(5),uar)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(acc)\n",
    "print(uar)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6195711929631665, 0.5919381557150746, 0.576310861423221, 0.54113171659534, 0.543778801843318]\n",
      "[0.5797473004493539, 0.5560675776078008, 0.6660561453766521, 0.5032713284393016, 0.489619002034886]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame([acc,uar]).T"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          0         1\n",
       "0  0.619571  0.579747\n",
       "1  0.591938  0.556068\n",
       "2  0.576311  0.666056\n",
       "3  0.541132  0.503271\n",
       "4  0.543779  0.489619"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.619571</td>\n",
       "      <td>0.579747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.591938</td>\n",
       "      <td>0.556068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.576311</td>\n",
       "      <td>0.666056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.541132</td>\n",
       "      <td>0.503271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.543779</td>\n",
       "      <td>0.489619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.average(acc)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.574546145708024"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.average(uar)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5589522707815988"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "b045124bcdd1c40b84b667cdfcc8f45f39bd89c671aa4fba75a85560739fc3dc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}